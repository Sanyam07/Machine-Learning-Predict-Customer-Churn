{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "# Introduction: Feature Engineering with Spark\n\nProblem: In `Feature Engineering`, we developed a pipeline for automated feature engineering using a dataset of customer transactions and label times. Running this pipeline on a single partition of customers takes about 15 minutes which means computing all of the features would require several days if done one at a time. \n\nSolution: Break the dataset into independent partitions of customers and run multiple subsets in parallel. This can be done using multiple processors on a single machine or a cluster of machines.\n\n## Spark with PySpark\n\n[Apache Spark](http://spark.apache.org) is a popular framework for distributed computed and large-data processing. It allows us to run computations in parallel either on a single machine, or distributed across a cluster of machines. In this notebook, we will run automated feature engineering in Featuretoolsusing Spark with the [PySpark library](http://spark.apache.org/docs/2.2.0/api/python/pyspark.html). \n\nThe first step is initializing Spark. We can use the `findspark` library to make sure that `pyspark` can find Spark in the Jupyter Notebook. This notebook assumes the Spark cluster is already running. To get started with a Spark cluster, refer to [this guide](https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/). \n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "# Initialize with Spark file location\n",
        "findspark.init(\u0027/usr/local/spark\u0027)\n",
        "\n",
        "import pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Set up Spark \n",
        "\n",
        "A `SparkContext` is the interface to a running Spark cluster. We pass in a number of parameters to the `SparkContext` using a `SparkConf` object. Namely, we\u0027ll turn on logging, tell Spark to use all cores on our 3 machines, and direct Spark to the location of the master (parent) node. \n",
        "\n",
        "Adjust the parameters depending on your cluster set up. I found [this guide](https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html) to be helpful in choosing the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_items([(\u0027spark.eventLog.enabled\u0027, \u0027True\u0027), (\u0027spark.eventLog.dir\u0027, \u0027tmp/\u0027), (\u0027spark.num.executors\u0027, \u00273\u0027), (\u0027spark.executor.memory\u0027, \u002712g\u0027), (\u0027spark.executor.cores\u0027, \u00274\u0027), (\u0027spark.master\u0027, \u0027spark://ip-172-31-8-174.ec2.internal:7077\u0027)])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conf \u003d pyspark.SparkConf()\n",
        "\n",
        "# Enable logging\n",
        "conf.set(\u0027spark.eventLog.enabled\u0027, True);\n",
        "conf.set(\u0027spark.eventLog.dir\u0027, \u0027tmp/\u0027);\n",
        "\n",
        "# Use all cores on all machines\n",
        "conf.set(\u0027spark.num.executors\u0027, 3)\n",
        "conf.set(\u0027spark.executor.memory\u0027, \u002712g\u0027)\n",
        "conf.set(\u0027spark.executor.cores\u0027, 4)\n",
        "\n",
        "# Set the parent\n",
        "conf.set(\u0027spark.master\u0027, \u0027spark://ip-172-31-8-174.ec2.internal:7077\u0027)\n",
        "conf.getAll()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Testing Spark \n",
        "\n",
        "Before we get to the feature engineering, we want to test if our cluster is running correctly. We\u0027ll instantiate a `Spark` cluster and run a simple program that calculates the value of pi. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        \u003cdiv\u003e\n",
              "            \u003cp\u003e\u003cb\u003eSparkContext\u003c/b\u003e\u003c/p\u003e\n",
              "\n",
              "            \u003cp\u003e\u003ca href\u003d\"http://ip-172-31-23-133.ec2.internal:4040\"\u003eSpark UI\u003c/a\u003e\u003c/p\u003e\n",
              "\n",
              "            \u003cdl\u003e\n",
              "              \u003cdt\u003eVersion\u003c/dt\u003e\n",
              "                \u003cdd\u003e\u003ccode\u003ev2.3.1\u003c/code\u003e\u003c/dd\u003e\n",
              "              \u003cdt\u003eMaster\u003c/dt\u003e\n",
              "                \u003cdd\u003e\u003ccode\u003espark://ip-172-31-23-133.ec2.internal:7077\u003c/code\u003e\u003c/dd\u003e\n",
              "              \u003cdt\u003eAppName\u003c/dt\u003e\n",
              "                \u003cdd\u003e\u003ccode\u003epi_calc\u003c/code\u003e\u003c/dd\u003e\n",
              "            \u003c/dl\u003e\n",
              "        \u003c/div\u003e\n",
              "        "
            ],
            "text/plain": [
              "\u003cSparkContext master\u003dspark://ip-172-31-23-133.ec2.internal:7077 appName\u003dpi_calc\u003e"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sc \u003d pyspark.SparkContext(appName\u003d\"pi_calc\", \n",
        "                          conf \u003d conf)\n",
        "sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.14152968\n"
          ]
        }
      ],
      "source": [
        "num_samples \u003d 100000000\n",
        "import random\n",
        "\n",
        "def inside(p):     \n",
        "  x, y \u003d random.random(), random.random()\n",
        "  return x*x + y*y \u003c 1\n",
        "\n",
        "# Parallelize counting samples inside circle using Spark\n",
        "count \u003d sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
        "pi \u003d 4 * count / num_samples\n",
        "print(pi)\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Spark Dashboards\n",
        "\n",
        "After starting the Spark cluster  from the command line- before running any of the code in the notebook - you can view a dashboard of the cluster at localhost:8080. This shows basic information such as the number of workers and the currently running or completed jobs.\n",
        "\n",
        "\n",
        "![](../images/spark_cluster_main.png)\n",
        "\n",
        "Once a `SparkContext` has been initialized, the job can be viewed at localhost:4040. This shows particular details such as the number of tasks completed and the directed acyclic graph of the operation. \n",
        "\n",
        "![](../images/stages.png)\n",
        "\n",
        "Using the web dashboard can be a helpful method to help debug your cluster. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Once the cluster is running correctly, we can move on to feature engineering. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Data Storage\n",
        "\n",
        "All of the reading and writing for running with Spark will happen through S3. The partitioned files are all on s3 and we can use `pandas.read_csv` to read directly from s3. To write to s3, we use the `s3fs` library (shown a little later). \n",
        "\n",
        "### Read in Data from S3\n",
        "\n",
        "Before running this code, make sure to authenticate with Amazon Web Services from the command line to access your files in S3. Run `aws configure` and then input the appropriate information. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "import pandas as pd\nimport featuretools as ft\nimport featuretools.variable_types as vtypes\n\npartition \u003d 20\ndirectory \u003d \u0027s3://customer-churn-spark/p\u0027 + str(partition)\ncutoff_times_file \u003d \u0027MS-31_labels.csv\u0027\n\n\n# Read in the data files\nmembers \u003d pd.read_csv(f\u0027{directory}/members.csv\u0027, \n                  parse_dates\u003d[\u0027registration_init_time\u0027], \n                  infer_datetime_format \u003d True, \n                  dtype \u003d {\u0027gender\u0027: \u0027category\u0027})\n\ntrans \u003d pd.read_csv(f\u0027{directory}/transactions.csv\u0027,\n                   parse_dates\u003d[\u0027transaction_date\u0027, \u0027membership_expire_date\u0027], \n                    infer_datetime_format \u003d True)\n\nlogs \u003d pd.read_csv(f\u0027{directory}/logs.csv\u0027, parse_dates \u003d [\u0027date\u0027])\n\ncutoff_times \u003d pd.read_csv(f\u0027{directory}/{cutoff_times_file}\u0027, parse_dates \u003d [\u0027cutoff_time\u0027])\ncutoff_times \u003d cutoff_times.drop_duplicates(subset \u003d [\u0027msno\u0027, \u0027cutoff_time\u0027])\n\n# Create empty entityset\nes \u003d ft.EntitySet(id \u003d \u0027customers\u0027)\n\n# Add the members parent table\nes.entity_from_dataframe(entity_id\u003d\u0027members\u0027, dataframe\u003dmembers,\n                         index \u003d \u0027msno\u0027, time_index \u003d \u0027registration_init_time\u0027, \n                         variable_types \u003d {\u0027city\u0027: vtypes.Categorical, \u0027bd\u0027: vtypes.Categorical,\n                                           \u0027registered_via\u0027: vtypes.Categorical})\n# Create new features in transactions\ntrans[\u0027price_difference\u0027] \u003d trans[\u0027plan_list_price\u0027] - trans[\u0027actual_amount_paid\u0027]\ntrans[\u0027planned_daily_price\u0027] \u003d trans[\u0027plan_list_price\u0027] / trans[\u0027payment_plan_days\u0027]\ntrans[\u0027daily_price\u0027] \u003d trans[\u0027actual_amount_paid\u0027] / trans[\u0027payment_plan_days\u0027]\n\n# Add the transactions child table\nes.entity_from_dataframe(entity_id\u003d\u0027transactions\u0027, dataframe\u003dtrans,\n                         index \u003d \u0027transactions_index\u0027, make_index \u003d True,\n                         time_index \u003d \u0027transaction_date\u0027, \n                         variable_types \u003d {\u0027payment_method_id\u0027: vtypes.Categorical, \n                                           \u0027is_auto_renew\u0027: vtypes.Boolean, \u0027is_cancel\u0027: vtypes.Boolean})\n\n# Add transactions interesting values\nes[\u0027transactions\u0027][\u0027is_cancel\u0027].interesting_values \u003d [0, 1]\nes[\u0027transactions\u0027][\u0027is_auto_renew\u0027].interesting_values \u003d [0, 1]\n\n# Create new features in logs\nlogs[\u0027total\u0027] \u003d logs[[\u0027num_25\u0027, \u0027num_50\u0027, \u0027num_75\u0027, \u0027num_985\u0027, \u0027num_100\u0027]].sum(axis \u003d 1)\nlogs[\u0027percent_100\u0027] \u003d logs[\u0027num_100\u0027] / logs[\u0027total\u0027]\nlogs[\u0027percent_unique\u0027] \u003d logs[\u0027num_unq\u0027] / logs[\u0027total\u0027]\n\n# Add the logs child table\nes.entity_from_dataframe(entity_id\u003d\u0027logs\u0027, dataframe\u003dlogs,\n                     index \u003d \u0027logs_index\u0027, make_index \u003d True,\n                     time_index \u003d \u0027date\u0027)\n\n# Add the relationships\nr_member_transactions \u003d ft.Relationship(es[\u0027members\u0027][\u0027msno\u0027], es[\u0027transactions\u0027][\u0027msno\u0027])\nr_member_logs \u003d ft.Relationship(es[\u0027members\u0027][\u0027msno\u0027], es[\u0027logs\u0027][\u0027msno\u0027])\nes.add_relationships([r_member_transactions, r_member_logs])\n\nes"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "## Custom Primitives\n\nBelow is a custom primitive we wrote (see the `Feature Engineering` notebook) for this dataset. It calculates the total amount of a quantity in the previous month."
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def total_previous_month(numeric, datetime, time):\n",
        "    \"\"\"Return total of `numeric` column in the month prior to `time`.\"\"\"\n",
        "    df \u003d pd.DataFrame({\u0027value\u0027: numeric, \u0027date\u0027: datetime})\n",
        "    previous_month \u003d time.month - 1\n",
        "    year \u003d time.year\n",
        "   \n",
        "    # Handle January\n",
        "    if previous_month \u003d\u003d 0:\n",
        "        previous_month \u003d 12\n",
        "        year \u003d time.year - 1\n",
        "        \n",
        "    # Filter data and sum up total\n",
        "    df \u003d df[(df[\u0027date\u0027].dt.month \u003d\u003d previous_month) \u0026 (df[\u0027date\u0027].dt.year \u003d\u003d year)]\n",
        "    total \u003d df[\u0027value\u0027].sum()\n",
        "    \n",
        "    return total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from featuretools.primitives import make_agg_primitive\n",
        "\n",
        "# Takes in a number and outputs a number\n",
        "total_previous \u003d make_agg_primitive(total_previous_month, input_types \u003d [ft.variable_types.Numeric,\n",
        "                                                                         ft.variable_types.Datetime],\n",
        "                                    return_type \u003d ft.variable_types.Numeric, \n",
        "                                    uses_calc_time \u003d True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Run Deep Feature Synthesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The first time we create the features, we use `ft.dfs` passing in the selected primitives, the target entity, the critical `cutoff_time`, the depth of the feature to stack, and several other parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Specify aggregation primitives\n",
        "agg_primitives \u003d [\u0027sum\u0027, \u0027time_since_last\u0027, \u0027avg_time_between\u0027, \u0027all\u0027, \u0027mode\u0027, \u0027num_unique\u0027, \u0027min\u0027, \u0027last\u0027, \n",
        "                  \u0027mean\u0027, \u0027percent_true\u0027, \u0027max\u0027, \u0027std\u0027, \u0027count\u0027, total_previous]\n",
        "# Specify transformation primitives\n",
        "trans_primitives \u003d [\u0027weekend\u0027, \u0027cum_sum\u0027, \u0027day\u0027, \u0027month\u0027, \u0027diff\u0027, \u0027time_since_previous\u0027]\n",
        "\n",
        "# Specify where primitives\n",
        "where_primitives \u003d [\u0027sum\u0027, \u0027mean\u0027, \u0027percent_true\u0027, \u0027all\u0027, \u0027any\u0027]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Built 248 features\n",
            "Elapsed: 17:17 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 274/274 chunks\n"
          ]
        }
      ],
      "source": [
        "# Run deep feature synthesis\n",
        "feature_matrix, feature_defs \u003d ft.dfs(entityset\u003des, target_entity\u003d\u0027members\u0027, \n",
        "                                      cutoff_time \u003d cutoff_times, \n",
        "                                      agg_primitives \u003d agg_primitives,\n",
        "                                      trans_primitives \u003d trans_primitives,\n",
        "                                      where_primitives \u003d where_primitives,\n",
        "                                      max_depth \u003d 2, features_only \u003d False,\n",
        "                                      chunk_size \u003d 100, n_jobs \u003d 1, verbose \u003d 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "These features definitions can be saved on disk. Every time we want to make the same exact features, we can just pass in these into the `ft.calculate_feature_matrix` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "ft.save_features(feature_defs, \u0027/data/churn/features.txt\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 248 features.\n"
          ]
        }
      ],
      "source": [
        "feature_defs \u003d ft.load_features(\u0027/data/churn/features.txt\u0027)\n",
        "print(f\u0027There are {len(feature_defs)} features.\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Writing Feature Matrix to S3 \n",
        "\n",
        "In order to save each feature matrix from a partition to the cloud, we\u0027ll write it directly to s3. For this we can use the `s3fs` (s3 file system) Python library. We first have to authenticate with aws by loading in the credentials and then we can upload our csv much the same as we would write any csv. We use the [`s3fs` library](https://s3fs.readthedocs.io/). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import s3fs\n",
        "\n",
        "# Credentials\n",
        "with open(\u0027/data/credentials.txt\u0027, \u0027r\u0027) as f:\n",
        "    info \u003d f.read().strip().split(\u0027,\u0027)\n",
        "    key \u003d info[0]\n",
        "    secret \u003d info[1]\n",
        "\n",
        "fs \u003d s3fs.S3FileSystem(key\u003dkey, secret\u003dsecret)\n",
        "\n",
        "# S3 directory\n",
        "directory \u003d \u0027s3://customer-churn-spark/p\u0027 + str(partition)\n",
        "\n",
        "# Encode in order to write to s3\n",
        "bytes_to_write \u003d feature_matrix.to_csv(None).encode()\n",
        "\n",
        "# Write to s3\n",
        "with fs.open(f\u0027{directory}/feature_matrix.csv\u0027, \u0027wb\u0027) as f:\n",
        "    f.write(bytes_to_write)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Partition to Feature Matrix Function\n",
        "\n",
        "The main function of this notebook is used to make features from a single partition. \n",
        "\n",
        "This function, `partition_to_feature_matrix`, does the following:\n",
        "\n",
        "1. Takes in the name of a partition \n",
        "2. Reads the data from s3\n",
        "3. Creates an entityset from the data\n",
        "4. Computes the feature matrix for the partition\n",
        "5. Saves the feature matrix to s3\n",
        "\n",
        "Because all reading and writing happens through S3, we don\u0027t have to worry about disc space or about putting a copy of the data on each machine. Instead, we can simply read from and write to the cloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "N_PARTITIONS \u003d 1000\n",
        "BASE_DIR \u003d \u0027s3://customer-churn-spark/\u0027\n",
        "    \n",
        "def partition_to_feature_matrix(partition, feature_defs \u003d feature_defs, \n",
        "                                cutoff_time_name \u003d \u0027MS-31_labels.csv\u0027, write \u003d True):\n",
        "    \"\"\"Take in a partition number, create a feature matrix, and save to Amazon S3\n",
        "    \n",
        "    Params\n",
        "    --------\n",
        "        partition (int): number of partition\n",
        "        feature_defs (list of ft features): features to make for the partition\n",
        "        cutoff_time_name (str): name of cutoff time file\n",
        "        write: (boolean): whether to write the data to S3. Defaults to True\n",
        "        \n",
        "    Return\n",
        "    --------\n",
        "        None: saves the feature matrix to Amazon S3\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    partition_dir \u003d BASE_DIR + \u0027p\u0027 + str(partition)\n",
        "    \n",
        "    # Read in the data files\n",
        "    members \u003d pd.read_csv(f\u0027{partition_dir}/members.csv\u0027, \n",
        "                      parse_dates\u003d[\u0027registration_init_time\u0027], \n",
        "                      infer_datetime_format \u003d True, \n",
        "                      dtype \u003d {\u0027gender\u0027: \u0027category\u0027})\n",
        "\n",
        "    trans \u003d pd.read_csv(f\u0027{partition_dir}/transactions.csv\u0027,\n",
        "                       parse_dates\u003d[\u0027transaction_date\u0027, \u0027membership_expire_date\u0027], \n",
        "                        infer_datetime_format \u003d True)\n",
        "    logs \u003d pd.read_csv(f\u0027{partition_dir}/logs.csv\u0027, parse_dates \u003d [\u0027date\u0027])\n",
        "    \n",
        "    # Make sure to drop duplicates\n",
        "    cutoff_times \u003d pd.read_csv(f\u0027{partition_dir}/{cutoff_time_name}\u0027, parse_dates \u003d [\u0027cutoff_time\u0027])\n",
        "    cutoff_times \u003d cutoff_times.drop_duplicates(subset \u003d [\u0027msno\u0027, \u0027cutoff_time\u0027])\n",
        "    \n",
        "    # Needed for saving\n",
        "    cutoff_spec \u003d cutoff_time_name.split(\u0027_\u0027)[0]\n",
        "    \n",
        "    # Create empty entityset\n",
        "    es \u003d ft.EntitySet(id \u003d \u0027customers\u0027)\n",
        "\n",
        "    # Add the members parent table\n",
        "    es.entity_from_dataframe(entity_id\u003d\u0027members\u0027, dataframe\u003dmembers,\n",
        "                             index \u003d \u0027msno\u0027, time_index \u003d \u0027registration_init_time\u0027, \n",
        "                             variable_types \u003d {\u0027city\u0027: vtypes.Categorical,\n",
        "                                               \u0027registered_via\u0027: vtypes.Categorical})\n",
        "    # Create new features in transactions\n",
        "    trans[\u0027price_difference\u0027] \u003d trans[\u0027plan_list_price\u0027] - trans[\u0027actual_amount_paid\u0027]\n",
        "    trans[\u0027planned_daily_price\u0027] \u003d trans[\u0027plan_list_price\u0027] / trans[\u0027payment_plan_days\u0027]\n",
        "    trans[\u0027daily_price\u0027] \u003d trans[\u0027actual_amount_paid\u0027] / trans[\u0027payment_plan_days\u0027]\n",
        "\n",
        "    # Add the transactions child table\n",
        "    es.entity_from_dataframe(entity_id\u003d\u0027transactions\u0027, dataframe\u003dtrans,\n",
        "                             index \u003d \u0027transactions_index\u0027, make_index \u003d True,\n",
        "                             time_index \u003d \u0027transaction_date\u0027, \n",
        "                             variable_types \u003d {\u0027payment_method_id\u0027: vtypes.Categorical, \n",
        "                                               \u0027is_auto_renew\u0027: vtypes.Boolean, \u0027is_cancel\u0027: vtypes.Boolean})\n",
        "\n",
        "    # Add transactions interesting values\n",
        "    es[\u0027transactions\u0027][\u0027is_cancel\u0027].interesting_values \u003d [0, 1]\n",
        "    es[\u0027transactions\u0027][\u0027is_auto_renew\u0027].interesting_values \u003d [0, 1]\n",
        "    \n",
        "    # Create new features in logs\n",
        "    logs[\u0027total\u0027] \u003d logs[[\u0027num_25\u0027, \u0027num_50\u0027, \u0027num_75\u0027, \u0027num_985\u0027, \u0027num_100\u0027]].sum(axis \u003d 1)\n",
        "    logs[\u0027percent_100\u0027] \u003d logs[\u0027num_100\u0027] / logs[\u0027total\u0027]\n",
        "    logs[\u0027percent_unique\u0027] \u003d logs[\u0027num_unq\u0027] / logs[\u0027total\u0027]\n",
        "    logs[\u0027seconds_per_song\u0027] \u003d logs[\u0027total_secs\u0027] / logs[\u0027total\u0027] \n",
        "    \n",
        "    # Add the logs child table\n",
        "    es.entity_from_dataframe(entity_id\u003d\u0027logs\u0027, dataframe\u003dlogs,\n",
        "                         index \u003d \u0027logs_index\u0027, make_index \u003d True,\n",
        "                         time_index \u003d \u0027date\u0027)\n",
        "\n",
        "    # Add the relationships\n",
        "    r_member_transactions \u003d ft.Relationship(es[\u0027members\u0027][\u0027msno\u0027], es[\u0027transactions\u0027][\u0027msno\u0027])\n",
        "    r_member_logs \u003d ft.Relationship(es[\u0027members\u0027][\u0027msno\u0027], es[\u0027logs\u0027][\u0027msno\u0027])\n",
        "    es.add_relationships([r_member_transactions, r_member_logs])\n",
        "    \n",
        "    # Calculate the feature matrix using pre-calculated features\n",
        "    feature_matrix \u003d ft.calculate_feature_matrix(entityset\u003des, features\u003dfeature_defs, \n",
        "                                                 cutoff_time\u003dcutoff_times, cutoff_time_in_index \u003d True,\n",
        "                                                 chunk_size \u003d 1000)\n",
        "\n",
        "    if write:\n",
        "        # Save to Amazon S3\n",
        "        bytes_to_write \u003d feature_matrix.to_csv(None).encode()\n",
        "\n",
        "        with fs.open(f\u0027{partition_dir}/{cutoff_spec}_feature_matrix.csv\u0027, \u0027wb\u0027) as f:\n",
        "            f.write(bytes_to_write)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Test Function\n",
        "\n",
        "Let\u0027s give the function a test with 2 different partitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "856 seconds elapsed.\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "start \u003d timer()\n",
        "partition_to_feature_matrix(950, feature_defs, \u0027MS-31_labels.csv\u0027)\n",
        "end \u003d timer()\n",
        "print(f\u0027{round(end - start)} seconds elapsed.\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\u003cdiv\u003e\n",
              "\u003cstyle scoped\u003e\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "\u003c/style\u003e\n",
              "\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n",
              "  \u003cthead\u003e\n",
              "    \u003ctr style\u003d\"text-align: right;\"\u003e\n",
              "      \u003cth\u003e\u003c/th\u003e\n",
              "      \u003cth\u003emsno\u003c/th\u003e\n",
              "      \u003cth\u003etime\u003c/th\u003e\n",
              "      \u003cth\u003ecity\u003c/th\u003e\n",
              "      \u003cth\u003ebd\u003c/th\u003e\n",
              "      \u003cth\u003eregistered_via\u003c/th\u003e\n",
              "      \u003cth\u003egender\u003c/th\u003e\n",
              "      \u003cth\u003eSUM(transactions.payment_plan_days)\u003c/th\u003e\n",
              "      \u003cth\u003eSUM(transactions.plan_list_price)\u003c/th\u003e\n",
              "      \u003cth\u003eSUM(transactions.actual_amount_paid)\u003c/th\u003e\n",
              "      \u003cth\u003eSUM(transactions.price_difference)\u003c/th\u003e\n",
              "      \u003cth\u003e...\u003c/th\u003e\n",
              "      \u003cth\u003eWEEKEND(LAST(logs.date))\u003c/th\u003e\n",
              "      \u003cth\u003eDAY(LAST(transactions.transaction_date))\u003c/th\u003e\n",
              "      \u003cth\u003eDAY(LAST(transactions.membership_expire_date))\u003c/th\u003e\n",
              "      \u003cth\u003eDAY(LAST(logs.date))\u003c/th\u003e\n",
              "      \u003cth\u003eMONTH(LAST(transactions.transaction_date))\u003c/th\u003e\n",
              "      \u003cth\u003eMONTH(LAST(transactions.membership_expire_date))\u003c/th\u003e\n",
              "      \u003cth\u003eMONTH(LAST(logs.date))\u003c/th\u003e\n",
              "      \u003cth\u003elabel\u003c/th\u003e\n",
              "      \u003cth\u003edays_to_churn\u003c/th\u003e\n",
              "      \u003cth\u003echurn_date\u003c/th\u003e\n",
              "    \u003c/tr\u003e\n",
              "  \u003c/thead\u003e\n",
              "  \u003ctbody\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e0\u003c/th\u003e\n",
              "      \u003ctd\u003e+8eyBkJyRyRK08Fu+mpDQ0/JljpCcOdPiWfOCuxqZWQ\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e15.0\u003c/td\u003e\n",
              "      \u003ctd\u003e32.0\u003c/td\u003e\n",
              "      \u003ctd\u003e9.0\u003c/td\u003e\n",
              "      \u003ctd\u003efemale\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e1\u003c/th\u003e\n",
              "      \u003ctd\u003e+BDZoGIRQSQHuzwSu5hiOJ6sVaQkEBixPBI42HhtvX8\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e22.0\u003c/td\u003e\n",
              "      \u003ctd\u003e29.0\u003c/td\u003e\n",
              "      \u003ctd\u003e9.0\u003c/td\u003e\n",
              "      \u003ctd\u003emale\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e472.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e2\u003c/th\u003e\n",
              "      \u003ctd\u003e+IETRNY5pdehTZK7HxJS55bUVmZpLCbkXjYdxplIt+c\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e22.0\u003c/td\u003e\n",
              "      \u003ctd\u003e32.0\u003c/td\u003e\n",
              "      \u003ctd\u003e9.0\u003c/td\u003e\n",
              "      \u003ctd\u003emale\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e459.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e3\u003c/th\u003e\n",
              "      \u003ctd\u003e+Ico+LLCU6UPRQAoS9Q+BDxkU+CyQvr44bjrWY4RJjI\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e7.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e4\u003c/th\u003e\n",
              "      \u003ctd\u003e+LO1Iu1Tc0Cz5rjcIo1CgZEqr3poGDFMhPLGA3uvWVo\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e6.0\u003c/td\u003e\n",
              "      \u003ctd\u003e21.0\u003c/td\u003e\n",
              "      \u003ctd\u003e9.0\u003c/td\u003e\n",
              "      \u003ctd\u003efemale\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e418.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "  \u003c/tbody\u003e\n",
              "\u003c/table\u003e\n",
              "\u003cp\u003e5 rows × 253 columns\u003c/p\u003e\n",
              "\u003c/div\u003e"
            ],
            "text/plain": [
              "                                           msno        time  city    bd  \\\n",
              "0  +8eyBkJyRyRK08Fu+mpDQ0/JljpCcOdPiWfOCuxqZWQ\u003d  2015-01-01  15.0  32.0   \n",
              "1  +BDZoGIRQSQHuzwSu5hiOJ6sVaQkEBixPBI42HhtvX8\u003d  2015-01-01  22.0  29.0   \n",
              "2  +IETRNY5pdehTZK7HxJS55bUVmZpLCbkXjYdxplIt+c\u003d  2015-01-01  22.0  32.0   \n",
              "3  +Ico+LLCU6UPRQAoS9Q+BDxkU+CyQvr44bjrWY4RJjI\u003d  2015-01-01   1.0   0.0   \n",
              "4  +LO1Iu1Tc0Cz5rjcIo1CgZEqr3poGDFMhPLGA3uvWVo\u003d  2015-01-01   6.0  21.0   \n",
              "\n",
              "   registered_via  gender  SUM(transactions.payment_plan_days)  \\\n",
              "0             9.0  female                                  0.0   \n",
              "1             9.0    male                                  0.0   \n",
              "2             9.0    male                                  0.0   \n",
              "3             7.0     NaN                                  0.0   \n",
              "4             9.0  female                                  0.0   \n",
              "\n",
              "   SUM(transactions.plan_list_price)  SUM(transactions.actual_amount_paid)  \\\n",
              "0                                0.0                                   0.0   \n",
              "1                                0.0                                   0.0   \n",
              "2                                0.0                                   0.0   \n",
              "3                                0.0                                   0.0   \n",
              "4                                0.0                                   0.0   \n",
              "\n",
              "   SUM(transactions.price_difference)     ...      WEEKEND(LAST(logs.date))  \\\n",
              "0                                 0.0     ...                           0.0   \n",
              "1                                 0.0     ...                           0.0   \n",
              "2                                 0.0     ...                           0.0   \n",
              "3                                 0.0     ...                           0.0   \n",
              "4                                 0.0     ...                           0.0   \n",
              "\n",
              "   DAY(LAST(transactions.transaction_date))  \\\n",
              "0                                       NaN   \n",
              "1                                       NaN   \n",
              "2                                       NaN   \n",
              "3                                       NaN   \n",
              "4                                       NaN   \n",
              "\n",
              "   DAY(LAST(transactions.membership_expire_date))  DAY(LAST(logs.date))  \\\n",
              "0                                             NaN                   1.0   \n",
              "1                                             NaN                   1.0   \n",
              "2                                             NaN                   1.0   \n",
              "3                                             NaN                   1.0   \n",
              "4                                             NaN                   1.0   \n",
              "\n",
              "  MONTH(LAST(transactions.transaction_date))  \\\n",
              "0                                        NaN   \n",
              "1                                        NaN   \n",
              "2                                        NaN   \n",
              "3                                        NaN   \n",
              "4                                        NaN   \n",
              "\n",
              "  MONTH(LAST(transactions.membership_expire_date))  MONTH(LAST(logs.date))  \\\n",
              "0                                              NaN                     1.0   \n",
              "1                                              NaN                     1.0   \n",
              "2                                              NaN                     1.0   \n",
              "3                                              NaN                     1.0   \n",
              "4                                              NaN                     1.0   \n",
              "\n",
              "   label  days_to_churn  churn_date  \n",
              "0    0.0            NaN         NaN  \n",
              "1    NaN          472.0         NaN  \n",
              "2    NaN          459.0         NaN  \n",
              "3    0.0            NaN         NaN  \n",
              "4    NaN          418.0         NaN  \n",
              "\n",
              "[5 rows x 253 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_matrix \u003d pd.read_csv(\u0027s3://customer-churn-spark/p950/MS-31_feature_matrix.csv\u0027, low_memory \u003d False)\n",
        "feature_matrix.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "875 seconds elapsed.\n"
          ]
        }
      ],
      "source": [
        "start \u003d timer()\n",
        "partition_to_feature_matrix(530, feature_defs, \u0027MS-31_labels.csv\u0027)\n",
        "end \u003d timer()\n",
        "print(f\u0027{round(end - start)} seconds elapsed.\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\u003cdiv\u003e\n",
              "\u003cstyle scoped\u003e\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "\u003c/style\u003e\n",
              "\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n",
              "  \u003cthead\u003e\n",
              "    \u003ctr style\u003d\"text-align: right;\"\u003e\n",
              "      \u003cth\u003e\u003c/th\u003e\n",
              "      \u003cth\u003emsno\u003c/th\u003e\n",
              "      \u003cth\u003etime\u003c/th\u003e\n",
              "      \u003cth\u003ecity\u003c/th\u003e\n",
              "      \u003cth\u003ebd\u003c/th\u003e\n",
              "      \u003cth\u003eregistered_via\u003c/th\u003e\n",
              "      \u003cth\u003egender\u003c/th\u003e\n",
              "      \u003cth\u003eSUM(transactions.payment_plan_days)\u003c/th\u003e\n",
              "      \u003cth\u003eSUM(transactions.plan_list_price)\u003c/th\u003e\n",
              "      \u003cth\u003eSUM(transactions.actual_amount_paid)\u003c/th\u003e\n",
              "      \u003cth\u003eSUM(transactions.price_difference)\u003c/th\u003e\n",
              "      \u003cth\u003e...\u003c/th\u003e\n",
              "      \u003cth\u003eWEEKEND(LAST(logs.date))\u003c/th\u003e\n",
              "      \u003cth\u003eDAY(LAST(transactions.transaction_date))\u003c/th\u003e\n",
              "      \u003cth\u003eDAY(LAST(transactions.membership_expire_date))\u003c/th\u003e\n",
              "      \u003cth\u003eDAY(LAST(logs.date))\u003c/th\u003e\n",
              "      \u003cth\u003eMONTH(LAST(transactions.transaction_date))\u003c/th\u003e\n",
              "      \u003cth\u003eMONTH(LAST(transactions.membership_expire_date))\u003c/th\u003e\n",
              "      \u003cth\u003eMONTH(LAST(logs.date))\u003c/th\u003e\n",
              "      \u003cth\u003elabel\u003c/th\u003e\n",
              "      \u003cth\u003edays_to_churn\u003c/th\u003e\n",
              "      \u003cth\u003echurn_date\u003c/th\u003e\n",
              "    \u003c/tr\u003e\n",
              "  \u003c/thead\u003e\n",
              "  \u003ctbody\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e0\u003c/th\u003e\n",
              "      \u003ctd\u003e+2oyBGdHsUwF9UZQAF6JFSlOwohoHPFriNBUQDzj6xw\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e14.0\u003c/td\u003e\n",
              "      \u003ctd\u003e49.0\u003c/td\u003e\n",
              "      \u003ctd\u003e9.0\u003c/td\u003e\n",
              "      \u003ctd\u003emale\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e1\u003c/th\u003e\n",
              "      \u003ctd\u003e+C8j6Pj/MCr/nAANcuJzta8lCkoZ6oopypdhllkqXlM\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e4.0\u003c/td\u003e\n",
              "      \u003ctd\u003e27.0\u003c/td\u003e\n",
              "      \u003ctd\u003e9.0\u003c/td\u003e\n",
              "      \u003ctd\u003emale\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e2\u003c/th\u003e\n",
              "      \u003ctd\u003e+E+cBkZzqIXPd4L1vLHYO1xxoD6VF7J5mi1Z/GKA9r0\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e7.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e3\u003c/th\u003e\n",
              "      \u003ctd\u003e+O51KSmGMnp+ItBpBgZNBJ94K/e//4fhXGYmxNHvZcg\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e13.0\u003c/td\u003e\n",
              "      \u003ctd\u003e39.0\u003c/td\u003e\n",
              "      \u003ctd\u003e9.0\u003c/td\u003e\n",
              "      \u003ctd\u003efemale\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e4\u003c/th\u003e\n",
              "      \u003ctd\u003e+WiZkfIp5sDsf0xZvBnR2j6Kxi1u2k0t0mJBJqhQIJo\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e13.0\u003c/td\u003e\n",
              "      \u003ctd\u003e25.0\u003c/td\u003e\n",
              "      \u003ctd\u003e9.0\u003c/td\u003e\n",
              "      \u003ctd\u003emale\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e472.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "  \u003c/tbody\u003e\n",
              "\u003c/table\u003e\n",
              "\u003cp\u003e5 rows × 253 columns\u003c/p\u003e\n",
              "\u003c/div\u003e"
            ],
            "text/plain": [
              "                                           msno        time  city    bd  \\\n",
              "0  +2oyBGdHsUwF9UZQAF6JFSlOwohoHPFriNBUQDzj6xw\u003d  2015-01-01  14.0  49.0   \n",
              "1  +C8j6Pj/MCr/nAANcuJzta8lCkoZ6oopypdhllkqXlM\u003d  2015-01-01   4.0  27.0   \n",
              "2  +E+cBkZzqIXPd4L1vLHYO1xxoD6VF7J5mi1Z/GKA9r0\u003d  2015-01-01   1.0   0.0   \n",
              "3  +O51KSmGMnp+ItBpBgZNBJ94K/e//4fhXGYmxNHvZcg\u003d  2015-01-01  13.0  39.0   \n",
              "4  +WiZkfIp5sDsf0xZvBnR2j6Kxi1u2k0t0mJBJqhQIJo\u003d  2015-01-01  13.0  25.0   \n",
              "\n",
              "   registered_via  gender  SUM(transactions.payment_plan_days)  \\\n",
              "0             9.0    male                                  0.0   \n",
              "1             9.0    male                                  0.0   \n",
              "2             7.0     NaN                                  0.0   \n",
              "3             9.0  female                                  0.0   \n",
              "4             9.0    male                                  0.0   \n",
              "\n",
              "   SUM(transactions.plan_list_price)  SUM(transactions.actual_amount_paid)  \\\n",
              "0                                0.0                                   0.0   \n",
              "1                                0.0                                   0.0   \n",
              "2                                0.0                                   0.0   \n",
              "3                                0.0                                   0.0   \n",
              "4                                0.0                                   0.0   \n",
              "\n",
              "   SUM(transactions.price_difference)     ...      WEEKEND(LAST(logs.date))  \\\n",
              "0                                 0.0     ...                           0.0   \n",
              "1                                 0.0     ...                           0.0   \n",
              "2                                 0.0     ...                           0.0   \n",
              "3                                 0.0     ...                           0.0   \n",
              "4                                 0.0     ...                           0.0   \n",
              "\n",
              "   DAY(LAST(transactions.transaction_date))  \\\n",
              "0                                       NaN   \n",
              "1                                       NaN   \n",
              "2                                       NaN   \n",
              "3                                       NaN   \n",
              "4                                       NaN   \n",
              "\n",
              "   DAY(LAST(transactions.membership_expire_date))  DAY(LAST(logs.date))  \\\n",
              "0                                             NaN                   NaN   \n",
              "1                                             NaN                   1.0   \n",
              "2                                             NaN                   1.0   \n",
              "3                                             NaN                   NaN   \n",
              "4                                             NaN                   1.0   \n",
              "\n",
              "  MONTH(LAST(transactions.transaction_date))  \\\n",
              "0                                        NaN   \n",
              "1                                        NaN   \n",
              "2                                        NaN   \n",
              "3                                        NaN   \n",
              "4                                        NaN   \n",
              "\n",
              "  MONTH(LAST(transactions.membership_expire_date))  MONTH(LAST(logs.date))  \\\n",
              "0                                              NaN                     NaN   \n",
              "1                                              NaN                     1.0   \n",
              "2                                              NaN                     1.0   \n",
              "3                                              NaN                     NaN   \n",
              "4                                              NaN                     1.0   \n",
              "\n",
              "   label  days_to_churn  churn_date  \n",
              "0    0.0            NaN         NaN  \n",
              "1    0.0            NaN         NaN  \n",
              "2    0.0            NaN         NaN  \n",
              "3    0.0            NaN         NaN  \n",
              "4    NaN          472.0         NaN  \n",
              "\n",
              "[5 rows x 253 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_matrix \u003d pd.read_csv(\u0027s3://customer-churn-spark/p530/MS-31_feature_matrix.csv\u0027, low_memory \u003d False)\n",
        "feature_matrix.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Run with Spark\n",
        "\n",
        "The next cell parallelizes all the feature engineering calculations using Spark. We want to `map` the partitions to the function and we let Spark divide the work between the executors, each of which is one core on one machine. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Create list of partitions\n",
        "partitions \u003d list(range(N_PARTITIONS))\n",
        "\n",
        "# Create Spark context\n",
        "sc \u003d pyspark.SparkContext(master \u003d \u0027spark://ip-172-31-8-174.ec2.internal:7077\u0027,\n",
        "                          appName \u003d \u0027featuretools\u0027, conf \u003d conf)\n",
        "\n",
        "# Parallelize feature engineering\n",
        "r \u003d sc.parallelize(partitions, numSlices\u003dN_PARTITIONS).\\\n",
        "    map(lambda x: partition_to_feature_matrix(x, feature_defs,\n",
        "                                              \u0027MS-31_labels.csv\u0027)).collect()\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "While the run is going on, we can look at the status of the cluster at localhost:8080 and the state of the particular job at localhost:4040. \n\n__Here is the overall state of the cluster.__\n\n![](../images/spark_cluster2.png)\n\n__Here is information about the submitted job.__\n\n![](../images/spark_job.png) "
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Joining the Data\n",
        "\n",
        "From here, we could read in all the partitioned feature matrices and build a single feature matrix, or if we have a model that supports [incremental (also known as on-line) learning](https://en.wikipedia.org/wiki/Incremental_learning), we can train it with one partition at a time. With all of the data in S3, we can access it from any machine which means we don\u0027t have to worry about losing data through stopping/starting machines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\u003cdiv\u003e\n",
              "\u003cstyle scoped\u003e\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "\u003c/style\u003e\n",
              "\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n",
              "  \u003cthead\u003e\n",
              "    \u003ctr style\u003d\"text-align: right;\"\u003e\n",
              "      \u003cth\u003e\u003c/th\u003e\n",
              "      \u003cth\u003emsno\u003c/th\u003e\n",
              "      \u003cth\u003etime\u003c/th\u003e\n",
              "      \u003cth\u003ecity\u003c/th\u003e\n",
              "      \u003cth\u003ebd\u003c/th\u003e\n",
              "      \u003cth\u003eregistered_via\u003c/th\u003e\n",
              "      \u003cth\u003egender\u003c/th\u003e\n",
              "      \u003cth\u003eSUM(logs.num_25)\u003c/th\u003e\n",
              "      \u003cth\u003eSUM(logs.num_50)\u003c/th\u003e\n",
              "      \u003cth\u003eSUM(logs.num_75)\u003c/th\u003e\n",
              "      \u003cth\u003eSUM(logs.num_985)\u003c/th\u003e\n",
              "      \u003cth\u003e...\u003c/th\u003e\n",
              "      \u003cth\u003eWEEKEND(LAST(transactions.membership_expire_date))\u003c/th\u003e\n",
              "      \u003cth\u003eDAY(LAST(logs.date))\u003c/th\u003e\n",
              "      \u003cth\u003eDAY(LAST(transactions.transaction_date))\u003c/th\u003e\n",
              "      \u003cth\u003eDAY(LAST(transactions.membership_expire_date))\u003c/th\u003e\n",
              "      \u003cth\u003eMONTH(LAST(logs.date))\u003c/th\u003e\n",
              "      \u003cth\u003eMONTH(LAST(transactions.transaction_date))\u003c/th\u003e\n",
              "      \u003cth\u003eMONTH(LAST(transactions.membership_expire_date))\u003c/th\u003e\n",
              "      \u003cth\u003elabel\u003c/th\u003e\n",
              "      \u003cth\u003edays_to_churn\u003c/th\u003e\n",
              "      \u003cth\u003echurn_date\u003c/th\u003e\n",
              "    \u003c/tr\u003e\n",
              "  \u003c/thead\u003e\n",
              "  \u003ctbody\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e0\u003c/th\u003e\n",
              "      \u003ctd\u003e++Q7h1vrVXrpWGnIDtttZ2O6bYGkF1fwOkAF5na5hF4\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e9.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e5.0\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e1\u003c/th\u003e\n",
              "      \u003ctd\u003e+KEJoiWQSU7N0XwM9XCa+wJNvyxhkO5g5uG1GAIWeRI\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e13.0\u003c/td\u003e\n",
              "      \u003ctd\u003e27.0\u003c/td\u003e\n",
              "      \u003ctd\u003e9.0\u003c/td\u003e\n",
              "      \u003ctd\u003efemale\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e451.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e2\u003c/th\u003e\n",
              "      \u003ctd\u003e+WbnkpvQ8I4qLjXiHeASos5MzW0qJby5mSBNbFkqGN4\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e7.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e2.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e3\u003c/th\u003e\n",
              "      \u003ctd\u003e+oDiKDaW9HDo7BV23lD2O2zAz3UQ2oY+CcDRAMX36Wc\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e7.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e4\u003c/th\u003e\n",
              "      \u003ctd\u003e+vMYXB1GFE6vREfLE2RzbLNU9JZSlJdXGAD82xxSh4o\u003d\u003c/td\u003e\n",
              "      \u003ctd\u003e2015-01-01\u003c/td\u003e\n",
              "      \u003ctd\u003e5.0\u003c/td\u003e\n",
              "      \u003ctd\u003e26.0\u003c/td\u003e\n",
              "      \u003ctd\u003e9.0\u003c/td\u003e\n",
              "      \u003ctd\u003efemale\u003c/td\u003e\n",
              "      \u003ctd\u003e25.0\u003c/td\u003e\n",
              "      \u003ctd\u003e7.0\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003e4.0\u003c/td\u003e\n",
              "      \u003ctd\u003e...\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e1.0\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003eNaN\u003c/td\u003e\n",
              "      \u003ctd\u003e0.0\u003c/td\u003e\n",
              "      \u003ctd\u003e218.0\u003c/td\u003e\n",
              "      \u003ctd\u003e0\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "  \u003c/tbody\u003e\n",
              "\u003c/table\u003e\n",
              "\u003cp\u003e5 rows × 253 columns\u003c/p\u003e\n",
              "\u003c/div\u003e"
            ],
            "text/plain": [
              "                                           msno        time  city    bd  \\\n",
              "0  ++Q7h1vrVXrpWGnIDtttZ2O6bYGkF1fwOkAF5na5hF4\u003d  2015-01-01   1.0   0.0   \n",
              "1  +KEJoiWQSU7N0XwM9XCa+wJNvyxhkO5g5uG1GAIWeRI\u003d  2015-01-01  13.0  27.0   \n",
              "2  +WbnkpvQ8I4qLjXiHeASos5MzW0qJby5mSBNbFkqGN4\u003d  2015-01-01   1.0   0.0   \n",
              "3  +oDiKDaW9HDo7BV23lD2O2zAz3UQ2oY+CcDRAMX36Wc\u003d  2015-01-01   1.0   0.0   \n",
              "4  +vMYXB1GFE6vREfLE2RzbLNU9JZSlJdXGAD82xxSh4o\u003d  2015-01-01   5.0  26.0   \n",
              "\n",
              "   registered_via  gender  SUM(logs.num_25)  SUM(logs.num_50)  \\\n",
              "0             9.0     NaN               5.0               1.0   \n",
              "1             9.0  female               0.0               0.0   \n",
              "2             7.0     NaN               1.0               0.0   \n",
              "3             7.0     NaN               0.0               0.0   \n",
              "4             9.0  female              25.0               7.0   \n",
              "\n",
              "   SUM(logs.num_75)  SUM(logs.num_985)     ...      \\\n",
              "0               0.0                0.0     ...       \n",
              "1               0.0                0.0     ...       \n",
              "2               0.0                2.0     ...       \n",
              "3               0.0                0.0     ...       \n",
              "4               1.0                4.0     ...       \n",
              "\n",
              "   WEEKEND(LAST(transactions.membership_expire_date))  DAY(LAST(logs.date))  \\\n",
              "0                                                0.0                    1.0   \n",
              "1                                                0.0                    NaN   \n",
              "2                                                0.0                    1.0   \n",
              "3                                                0.0                    NaN   \n",
              "4                                                0.0                    1.0   \n",
              "\n",
              "   DAY(LAST(transactions.transaction_date))  \\\n",
              "0                                       NaN   \n",
              "1                                       NaN   \n",
              "2                                       NaN   \n",
              "3                                       NaN   \n",
              "4                                       NaN   \n",
              "\n",
              "   DAY(LAST(transactions.membership_expire_date))  MONTH(LAST(logs.date))  \\\n",
              "0                                             NaN                     1.0   \n",
              "1                                             NaN                     NaN   \n",
              "2                                             NaN                     1.0   \n",
              "3                                             NaN                     NaN   \n",
              "4                                             NaN                     1.0   \n",
              "\n",
              "   MONTH(LAST(transactions.transaction_date))  \\\n",
              "0                                         NaN   \n",
              "1                                         NaN   \n",
              "2                                         NaN   \n",
              "3                                         NaN   \n",
              "4                                         NaN   \n",
              "\n",
              "   MONTH(LAST(transactions.membership_expire_date))  label  days_to_churn  \\\n",
              "0                                               NaN    0.0            NaN   \n",
              "1                                               NaN    0.0          451.0   \n",
              "2                                               NaN    0.0            NaN   \n",
              "3                                               NaN    0.0            NaN   \n",
              "4                                               NaN    0.0          218.0   \n",
              "\n",
              "   churn_date  \n",
              "0         NaN  \n",
              "1           0  \n",
              "2         NaN  \n",
              "3         NaN  \n",
              "4           0  \n",
              "\n",
              "[5 rows x 253 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_matrix \u003d pd.read_csv(\u0027s3://customer-churn-spark/p999/MS-31_feature_matrix.csv\u0027, low_memory \u003d False)\n",
        "feature_matrix.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "In this notebook, we saw how to distribute feature engineering in Featuretools using the Spark framework. This big-data processing technology lets us use multiple computers to parallelize calculations, resulting in efficient data science workflows even on large datasets. \n",
        "\n",
        "The basic approach is:\n",
        "\n",
        "1. Divide data into independent partitions\n",
        "2. Run each subset in parallel with a different worker\n",
        "3. Join results together if necessary \n",
        "\n",
        "The nice part about using frameworks such as Dask and Spark with PySpark is we don\u0027t have to change the underlying Featuretools code. We write our code in native Python, change the backend running the calculations, and distribute the calculations across a cluster of machines. Using this approach, we\u0027ll be able to scale to any size datasets and take on even more exciting data science and machine learning problems. \n",
        "\n",
        "## Next Steps\n",
        "\n",
        "The final step of the machine learning pipeline is to build a model to make predictions for these features. This is implemented in the `Modeling` notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}